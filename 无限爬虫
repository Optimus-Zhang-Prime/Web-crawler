import urllib.request
from re import findall, S
from ssl import _create_unverified_context
from random import choice
from collections import deque
from urllib import error
import socket

def crawler(url, topath, equal=".*", urlqueue=deque()):
    user_agent = [
        "Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50",
        "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50",
        "Mozilla/5.0 (Windows NT 10.0; WOW64; rv:38.0) Gecko/20100101 Firefox/38.0",
        "Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; .NET4.0C; .NET4.0E; .NET CLR 2.0.50727; .NET CLR 3.0.30729; .NET CLR 3.5.30729; InfoPath.3; rv:11.0) like Gecko",
        "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)",
        "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
        "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
        "Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
        "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11",
        "Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser)",
        "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)"]
    head = {'User-Agent': choice(user_agent)}  # 随机产生请求头
    req = urllib.request.Request(url, headers=head)
    context1 = _create_unverified_context()
    try:
        response = urllib.request.urlopen(req, context=context1, timeout=3)
        try:
            data = response.read().decode("utf-8")
        except:
            data = response.read().decode("gbk")
        datalist = findall(equal, data, flags=S)
        for item in datalist:
            if len(item) == 0 or item == ' ':
                datalist.remove(item)
        print("得到数据的个数为：", len(datalist))
        for datatuple in datalist:
            urlqueue.append(datatuple[0])
    except error.HTTPError as e:
        print("http错误")
        print("当前网址为：", url)
    except socket.timeout as e:
        print("超时错误")
        print("当前网址为：", url)
    newurl = urlqueue.popleft()
    crawler(newurl, topath, equal, urlqueue)

if __name__ == "__main__":
    # 网址
    url = "https://www.baidu.com/?tn=62095104_19_oem_dg"
    # 存入文件路径
    topath = r"C:\Users\14531\PycharmProjects\test\bai.html"
    # 正则表达式
    equal = r'(((http|ftp|https)://)(([a-zA-Z0-9\._-]+\.[a-zA-Z]{2,6})' \
            r'|([0-9]{1,3}\.[0-9]{1-3}\.[0-9]{1,3}\.[0-9]{1,3}))(:[0-9]' \
            r'{1,4})*(/[a-zA-Z0-9\&%_\./-~-]*)?)'  # 匹配所有网址
    crawler(url, topath, equal)  # 调用爬虫
# with open(topath, "wb")as file:
